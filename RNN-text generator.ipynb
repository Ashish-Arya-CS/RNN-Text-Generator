{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# open text file and read in data as `text`\n",
    "with open('data/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([68, 61, 32, 30, 65, 38, 29,  9, 50, 11, 11, 11,  5, 32, 30, 30, 75,\n",
       "        9, 14, 32, 24, 39,  0, 39, 38, 59,  9, 32, 29, 38,  9, 32,  0,  0,\n",
       "        9, 32,  0, 39, 67, 38, 81,  9, 38, 77, 38, 29, 75,  9, 35,  7, 61,\n",
       "       32, 30, 30, 75,  9, 14, 32, 24, 39,  0, 75,  9, 39, 59,  9, 35,  7,\n",
       "       61, 32, 30, 30, 75,  9, 39,  7,  9, 39, 65, 59,  9, 76, 18,  7, 11,\n",
       "       18, 32, 75,  4, 11, 11, 48, 77, 38, 29, 75, 65, 61, 39,  7])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# check that the function works as expected\n",
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr,batch_size,seq_length):\n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "       \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        \n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        \n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 3.2504... Val Loss: 3.1824\n",
      "Epoch: 1/20... Step: 20... Loss: 3.1375... Val Loss: 3.1305\n",
      "Epoch: 1/20... Step: 30... Loss: 3.1370... Val Loss: 3.1207\n",
      "Epoch: 1/20... Step: 40... Loss: 3.1095... Val Loss: 3.1184\n",
      "Epoch: 1/20... Step: 50... Loss: 3.1384... Val Loss: 3.1167\n",
      "Epoch: 1/20... Step: 60... Loss: 3.1148... Val Loss: 3.1133\n",
      "Epoch: 1/20... Step: 70... Loss: 3.1016... Val Loss: 3.1070\n",
      "Epoch: 1/20... Step: 80... Loss: 3.1038... Val Loss: 3.0915\n",
      "Epoch: 1/20... Step: 90... Loss: 3.0734... Val Loss: 3.0569\n",
      "Epoch: 1/20... Step: 100... Loss: 3.0148... Val Loss: 2.9796\n",
      "Epoch: 1/20... Step: 110... Loss: 2.9094... Val Loss: 2.8793\n",
      "Epoch: 1/20... Step: 120... Loss: 2.8318... Val Loss: 2.8203\n",
      "Epoch: 1/20... Step: 130... Loss: 2.7828... Val Loss: 2.7476\n",
      "Epoch: 2/20... Step: 140... Loss: 2.7091... Val Loss: 2.6658\n",
      "Epoch: 2/20... Step: 150... Loss: 2.6118... Val Loss: 2.5841\n",
      "Epoch: 2/20... Step: 160... Loss: 2.5379... Val Loss: 2.5023\n",
      "Epoch: 2/20... Step: 170... Loss: 2.4655... Val Loss: 2.4555\n",
      "Epoch: 2/20... Step: 180... Loss: 2.4391... Val Loss: 2.4174\n",
      "Epoch: 2/20... Step: 190... Loss: 2.3943... Val Loss: 2.3805\n",
      "Epoch: 2/20... Step: 200... Loss: 2.3814... Val Loss: 2.3505\n",
      "Epoch: 2/20... Step: 210... Loss: 2.3465... Val Loss: 2.3276\n",
      "Epoch: 2/20... Step: 220... Loss: 2.3037... Val Loss: 2.2917\n",
      "Epoch: 2/20... Step: 230... Loss: 2.3005... Val Loss: 2.2843\n",
      "Epoch: 2/20... Step: 240... Loss: 2.2822... Val Loss: 2.2474\n",
      "Epoch: 2/20... Step: 250... Loss: 2.2119... Val Loss: 2.2172\n",
      "Epoch: 2/20... Step: 260... Loss: 2.1897... Val Loss: 2.1869\n",
      "Epoch: 2/20... Step: 270... Loss: 2.1913... Val Loss: 2.1661\n",
      "Epoch: 3/20... Step: 280... Loss: 2.1976... Val Loss: 2.1471\n",
      "Epoch: 3/20... Step: 290... Loss: 2.1610... Val Loss: 2.1202\n",
      "Epoch: 3/20... Step: 300... Loss: 2.1354... Val Loss: 2.1021\n",
      "Epoch: 3/20... Step: 310... Loss: 2.1027... Val Loss: 2.0788\n",
      "Epoch: 3/20... Step: 320... Loss: 2.0765... Val Loss: 2.0590\n",
      "Epoch: 3/20... Step: 330... Loss: 2.0523... Val Loss: 2.0496\n",
      "Epoch: 3/20... Step: 340... Loss: 2.0694... Val Loss: 2.0282\n",
      "Epoch: 3/20... Step: 350... Loss: 2.0554... Val Loss: 2.0079\n",
      "Epoch: 3/20... Step: 360... Loss: 1.9820... Val Loss: 1.9919\n",
      "Epoch: 3/20... Step: 370... Loss: 2.0076... Val Loss: 1.9757\n",
      "Epoch: 3/20... Step: 380... Loss: 1.9883... Val Loss: 1.9569\n",
      "Epoch: 3/20... Step: 390... Loss: 1.9591... Val Loss: 1.9430\n",
      "Epoch: 3/20... Step: 400... Loss: 1.9338... Val Loss: 1.9300\n",
      "Epoch: 3/20... Step: 410... Loss: 1.9431... Val Loss: 1.9154\n",
      "Epoch: 4/20... Step: 420... Loss: 1.9343... Val Loss: 1.9048\n",
      "Epoch: 4/20... Step: 430... Loss: 1.9315... Val Loss: 1.8894\n",
      "Epoch: 4/20... Step: 440... Loss: 1.9128... Val Loss: 1.8864\n",
      "Epoch: 4/20... Step: 450... Loss: 1.8416... Val Loss: 1.8607\n",
      "Epoch: 4/20... Step: 460... Loss: 1.8430... Val Loss: 1.8540\n",
      "Epoch: 4/20... Step: 470... Loss: 1.8823... Val Loss: 1.8447\n",
      "Epoch: 4/20... Step: 480... Loss: 1.8479... Val Loss: 1.8308\n",
      "Epoch: 4/20... Step: 490... Loss: 1.8490... Val Loss: 1.8186\n",
      "Epoch: 4/20... Step: 500... Loss: 1.8475... Val Loss: 1.8071\n",
      "Epoch: 4/20... Step: 510... Loss: 1.8207... Val Loss: 1.7971\n",
      "Epoch: 4/20... Step: 520... Loss: 1.8425... Val Loss: 1.7888\n",
      "Epoch: 4/20... Step: 530... Loss: 1.7969... Val Loss: 1.7837\n",
      "Epoch: 4/20... Step: 540... Loss: 1.7626... Val Loss: 1.7724\n",
      "Epoch: 4/20... Step: 550... Loss: 1.8103... Val Loss: 1.7575\n",
      "Epoch: 5/20... Step: 560... Loss: 1.7710... Val Loss: 1.7504\n",
      "Epoch: 5/20... Step: 570... Loss: 1.7576... Val Loss: 1.7391\n",
      "Epoch: 5/20... Step: 580... Loss: 1.7459... Val Loss: 1.7280\n",
      "Epoch: 5/20... Step: 590... Loss: 1.7453... Val Loss: 1.7228\n",
      "Epoch: 5/20... Step: 600... Loss: 1.7309... Val Loss: 1.7153\n",
      "Epoch: 5/20... Step: 610... Loss: 1.7107... Val Loss: 1.7093\n",
      "Epoch: 5/20... Step: 620... Loss: 1.7223... Val Loss: 1.6995\n",
      "Epoch: 5/20... Step: 630... Loss: 1.7341... Val Loss: 1.6946\n",
      "Epoch: 5/20... Step: 640... Loss: 1.7033... Val Loss: 1.6853\n",
      "Epoch: 5/20... Step: 650... Loss: 1.6979... Val Loss: 1.6799\n",
      "Epoch: 5/20... Step: 660... Loss: 1.6657... Val Loss: 1.6725\n",
      "Epoch: 5/20... Step: 670... Loss: 1.6965... Val Loss: 1.6669\n",
      "Epoch: 5/20... Step: 680... Loss: 1.6920... Val Loss: 1.6582\n",
      "Epoch: 5/20... Step: 690... Loss: 1.6644... Val Loss: 1.6531\n",
      "Epoch: 6/20... Step: 700... Loss: 1.6686... Val Loss: 1.6488\n",
      "Epoch: 6/20... Step: 710... Loss: 1.6552... Val Loss: 1.6421\n",
      "Epoch: 6/20... Step: 720... Loss: 1.6439... Val Loss: 1.6356\n",
      "Epoch: 6/20... Step: 730... Loss: 1.6646... Val Loss: 1.6303\n",
      "Epoch: 6/20... Step: 740... Loss: 1.6220... Val Loss: 1.6233\n",
      "Epoch: 6/20... Step: 750... Loss: 1.6139... Val Loss: 1.6173\n",
      "Epoch: 6/20... Step: 760... Loss: 1.6444... Val Loss: 1.6122\n",
      "Epoch: 6/20... Step: 770... Loss: 1.6334... Val Loss: 1.6129\n",
      "Epoch: 6/20... Step: 780... Loss: 1.6055... Val Loss: 1.6066\n",
      "Epoch: 6/20... Step: 790... Loss: 1.5941... Val Loss: 1.5987\n",
      "Epoch: 6/20... Step: 800... Loss: 1.6167... Val Loss: 1.5948\n",
      "Epoch: 6/20... Step: 810... Loss: 1.5992... Val Loss: 1.5887\n",
      "Epoch: 6/20... Step: 820... Loss: 1.5654... Val Loss: 1.5859\n",
      "Epoch: 6/20... Step: 830... Loss: 1.6141... Val Loss: 1.5797\n",
      "Epoch: 7/20... Step: 840... Loss: 1.5610... Val Loss: 1.5764\n",
      "Epoch: 7/20... Step: 850... Loss: 1.5780... Val Loss: 1.5704\n",
      "Epoch: 7/20... Step: 860... Loss: 1.5613... Val Loss: 1.5652\n",
      "Epoch: 7/20... Step: 870... Loss: 1.5739... Val Loss: 1.5599\n",
      "Epoch: 7/20... Step: 880... Loss: 1.5698... Val Loss: 1.5573\n",
      "Epoch: 7/20... Step: 890... Loss: 1.5661... Val Loss: 1.5536\n",
      "Epoch: 7/20... Step: 900... Loss: 1.5480... Val Loss: 1.5473\n",
      "Epoch: 7/20... Step: 910... Loss: 1.5254... Val Loss: 1.5484\n",
      "Epoch: 7/20... Step: 920... Loss: 1.5534... Val Loss: 1.5463\n",
      "Epoch: 7/20... Step: 930... Loss: 1.5236... Val Loss: 1.5390\n",
      "Epoch: 7/20... Step: 940... Loss: 1.5444... Val Loss: 1.5352\n",
      "Epoch: 7/20... Step: 950... Loss: 1.5483... Val Loss: 1.5280\n",
      "Epoch: 7/20... Step: 960... Loss: 1.5401... Val Loss: 1.5275\n",
      "Epoch: 7/20... Step: 970... Loss: 1.5558... Val Loss: 1.5238\n",
      "Epoch: 8/20... Step: 980... Loss: 1.5197... Val Loss: 1.5231\n",
      "Epoch: 8/20... Step: 990... Loss: 1.5275... Val Loss: 1.5164\n",
      "Epoch: 8/20... Step: 1000... Loss: 1.5153... Val Loss: 1.5121\n",
      "Epoch: 8/20... Step: 1010... Loss: 1.5592... Val Loss: 1.5107\n",
      "Epoch: 8/20... Step: 1020... Loss: 1.5271... Val Loss: 1.5050\n",
      "Epoch: 8/20... Step: 1030... Loss: 1.4979... Val Loss: 1.5017\n",
      "Epoch: 8/20... Step: 1040... Loss: 1.5053... Val Loss: 1.5040\n",
      "Epoch: 8/20... Step: 1050... Loss: 1.4971... Val Loss: 1.5000\n",
      "Epoch: 8/20... Step: 1060... Loss: 1.4943... Val Loss: 1.4977\n",
      "Epoch: 8/20... Step: 1070... Loss: 1.5019... Val Loss: 1.4924\n",
      "Epoch: 8/20... Step: 1080... Loss: 1.5033... Val Loss: 1.4880\n",
      "Epoch: 8/20... Step: 1090... Loss: 1.4831... Val Loss: 1.4830\n",
      "Epoch: 8/20... Step: 1100... Loss: 1.4661... Val Loss: 1.4805\n",
      "Epoch: 8/20... Step: 1110... Loss: 1.4740... Val Loss: 1.4782\n",
      "Epoch: 9/20... Step: 1120... Loss: 1.4835... Val Loss: 1.4794\n",
      "Epoch: 9/20... Step: 1130... Loss: 1.4829... Val Loss: 1.4737\n",
      "Epoch: 9/20... Step: 1140... Loss: 1.4896... Val Loss: 1.4716\n",
      "Epoch: 9/20... Step: 1150... Loss: 1.4962... Val Loss: 1.4746\n",
      "Epoch: 9/20... Step: 1160... Loss: 1.4474... Val Loss: 1.4660\n",
      "Epoch: 9/20... Step: 1170... Loss: 1.4625... Val Loss: 1.4647\n",
      "Epoch: 9/20... Step: 1180... Loss: 1.4561... Val Loss: 1.4679\n",
      "Epoch: 9/20... Step: 1190... Loss: 1.4881... Val Loss: 1.4630\n",
      "Epoch: 9/20... Step: 1200... Loss: 1.4458... Val Loss: 1.4587\n",
      "Epoch: 9/20... Step: 1210... Loss: 1.4436... Val Loss: 1.4533\n",
      "Epoch: 9/20... Step: 1220... Loss: 1.4552... Val Loss: 1.4562\n",
      "Epoch: 9/20... Step: 1230... Loss: 1.4302... Val Loss: 1.4502\n",
      "Epoch: 9/20... Step: 1240... Loss: 1.4447... Val Loss: 1.4472\n",
      "Epoch: 9/20... Step: 1250... Loss: 1.4476... Val Loss: 1.4452\n",
      "Epoch: 10/20... Step: 1260... Loss: 1.4521... Val Loss: 1.4458\n",
      "Epoch: 10/20... Step: 1270... Loss: 1.4455... Val Loss: 1.4420\n",
      "Epoch: 10/20... Step: 1280... Loss: 1.4477... Val Loss: 1.4368\n",
      "Epoch: 10/20... Step: 1290... Loss: 1.4378... Val Loss: 1.4414\n",
      "Epoch: 10/20... Step: 1300... Loss: 1.4273... Val Loss: 1.4353\n",
      "Epoch: 10/20... Step: 1310... Loss: 1.4350... Val Loss: 1.4324\n",
      "Epoch: 10/20... Step: 1320... Loss: 1.4105... Val Loss: 1.4345\n",
      "Epoch: 10/20... Step: 1330... Loss: 1.4229... Val Loss: 1.4321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20... Step: 1340... Loss: 1.4091... Val Loss: 1.4282\n",
      "Epoch: 10/20... Step: 1350... Loss: 1.3976... Val Loss: 1.4256\n",
      "Epoch: 10/20... Step: 1360... Loss: 1.3950... Val Loss: 1.4254\n",
      "Epoch: 10/20... Step: 1370... Loss: 1.3921... Val Loss: 1.4225\n",
      "Epoch: 10/20... Step: 1380... Loss: 1.4316... Val Loss: 1.4206\n",
      "Epoch: 10/20... Step: 1390... Loss: 1.4340... Val Loss: 1.4210\n",
      "Epoch: 11/20... Step: 1400... Loss: 1.4375... Val Loss: 1.4174\n",
      "Epoch: 11/20... Step: 1410... Loss: 1.4463... Val Loss: 1.4178\n",
      "Epoch: 11/20... Step: 1420... Loss: 1.4273... Val Loss: 1.4109\n",
      "Epoch: 11/20... Step: 1430... Loss: 1.4042... Val Loss: 1.4172\n",
      "Epoch: 11/20... Step: 1440... Loss: 1.4345... Val Loss: 1.4105\n",
      "Epoch: 11/20... Step: 1450... Loss: 1.3577... Val Loss: 1.4102\n",
      "Epoch: 11/20... Step: 1460... Loss: 1.3852... Val Loss: 1.4056\n",
      "Epoch: 11/20... Step: 1470... Loss: 1.3701... Val Loss: 1.4073\n",
      "Epoch: 11/20... Step: 1480... Loss: 1.3934... Val Loss: 1.4035\n",
      "Epoch: 11/20... Step: 1490... Loss: 1.3857... Val Loss: 1.4005\n",
      "Epoch: 11/20... Step: 1500... Loss: 1.3737... Val Loss: 1.4021\n",
      "Epoch: 11/20... Step: 1510... Loss: 1.3511... Val Loss: 1.4019\n",
      "Epoch: 11/20... Step: 1520... Loss: 1.3880... Val Loss: 1.3965\n",
      "Epoch: 12/20... Step: 1530... Loss: 1.4472... Val Loss: 1.3977\n",
      "Epoch: 12/20... Step: 1540... Loss: 1.4040... Val Loss: 1.3948\n",
      "Epoch: 12/20... Step: 1550... Loss: 1.3986... Val Loss: 1.3955\n",
      "Epoch: 12/20... Step: 1560... Loss: 1.4118... Val Loss: 1.3899\n",
      "Epoch: 12/20... Step: 1570... Loss: 1.3612... Val Loss: 1.3950\n",
      "Epoch: 12/20... Step: 1580... Loss: 1.3308... Val Loss: 1.3930\n",
      "Epoch: 12/20... Step: 1590... Loss: 1.3326... Val Loss: 1.3922\n",
      "Epoch: 12/20... Step: 1600... Loss: 1.3673... Val Loss: 1.3884\n",
      "Epoch: 12/20... Step: 1610... Loss: 1.3463... Val Loss: 1.3900\n",
      "Epoch: 12/20... Step: 1620... Loss: 1.3501... Val Loss: 1.3831\n",
      "Epoch: 12/20... Step: 1630... Loss: 1.3679... Val Loss: 1.3821\n",
      "Epoch: 12/20... Step: 1640... Loss: 1.3487... Val Loss: 1.3847\n",
      "Epoch: 12/20... Step: 1650... Loss: 1.3314... Val Loss: 1.3837\n",
      "Epoch: 12/20... Step: 1660... Loss: 1.3727... Val Loss: 1.3772\n",
      "Epoch: 13/20... Step: 1670... Loss: 1.3561... Val Loss: 1.3816\n",
      "Epoch: 13/20... Step: 1680... Loss: 1.3589... Val Loss: 1.3790\n",
      "Epoch: 13/20... Step: 1690... Loss: 1.3466... Val Loss: 1.3772\n",
      "Epoch: 13/20... Step: 1700... Loss: 1.3519... Val Loss: 1.3739\n",
      "Epoch: 13/20... Step: 1710... Loss: 1.3180... Val Loss: 1.3791\n",
      "Epoch: 13/20... Step: 1720... Loss: 1.3310... Val Loss: 1.3756\n",
      "Epoch: 13/20... Step: 1730... Loss: 1.3709... Val Loss: 1.3725\n",
      "Epoch: 13/20... Step: 1740... Loss: 1.3378... Val Loss: 1.3735\n",
      "Epoch: 13/20... Step: 1750... Loss: 1.3077... Val Loss: 1.3735\n",
      "Epoch: 13/20... Step: 1760... Loss: 1.3285... Val Loss: 1.3700\n",
      "Epoch: 13/20... Step: 1770... Loss: 1.3489... Val Loss: 1.3693\n",
      "Epoch: 13/20... Step: 1780... Loss: 1.3327... Val Loss: 1.3675\n",
      "Epoch: 13/20... Step: 1790... Loss: 1.3151... Val Loss: 1.3658\n",
      "Epoch: 13/20... Step: 1800... Loss: 1.3402... Val Loss: 1.3643\n",
      "Epoch: 14/20... Step: 1810... Loss: 1.3406... Val Loss: 1.3653\n",
      "Epoch: 14/20... Step: 1820... Loss: 1.3316... Val Loss: 1.3640\n",
      "Epoch: 14/20... Step: 1830... Loss: 1.3475... Val Loss: 1.3628\n",
      "Epoch: 14/20... Step: 1840... Loss: 1.3000... Val Loss: 1.3599\n",
      "Epoch: 14/20... Step: 1850... Loss: 1.2777... Val Loss: 1.3631\n",
      "Epoch: 14/20... Step: 1860... Loss: 1.3405... Val Loss: 1.3600\n",
      "Epoch: 14/20... Step: 1870... Loss: 1.3412... Val Loss: 1.3567\n",
      "Epoch: 14/20... Step: 1880... Loss: 1.3317... Val Loss: 1.3567\n",
      "Epoch: 14/20... Step: 1890... Loss: 1.3453... Val Loss: 1.3594\n",
      "Epoch: 14/20... Step: 1900... Loss: 1.3266... Val Loss: 1.3540\n",
      "Epoch: 14/20... Step: 1910... Loss: 1.3275... Val Loss: 1.3520\n",
      "Epoch: 14/20... Step: 1920... Loss: 1.3188... Val Loss: 1.3538\n",
      "Epoch: 14/20... Step: 1930... Loss: 1.2815... Val Loss: 1.3542\n",
      "Epoch: 14/20... Step: 1940... Loss: 1.3455... Val Loss: 1.3529\n",
      "Epoch: 15/20... Step: 1950... Loss: 1.3148... Val Loss: 1.3538\n",
      "Epoch: 15/20... Step: 1960... Loss: 1.3152... Val Loss: 1.3505\n",
      "Epoch: 15/20... Step: 1970... Loss: 1.3143... Val Loss: 1.3474\n",
      "Epoch: 15/20... Step: 1980... Loss: 1.2922... Val Loss: 1.3496\n",
      "Epoch: 15/20... Step: 1990... Loss: 1.2968... Val Loss: 1.3495\n",
      "Epoch: 15/20... Step: 2000... Loss: 1.2763... Val Loss: 1.3478\n",
      "Epoch: 15/20... Step: 2010... Loss: 1.3021... Val Loss: 1.3454\n",
      "Epoch: 15/20... Step: 2020... Loss: 1.3142... Val Loss: 1.3468\n",
      "Epoch: 15/20... Step: 2030... Loss: 1.2788... Val Loss: 1.3457\n",
      "Epoch: 15/20... Step: 2040... Loss: 1.3079... Val Loss: 1.3404\n",
      "Epoch: 15/20... Step: 2050... Loss: 1.2784... Val Loss: 1.3418\n",
      "Epoch: 15/20... Step: 2060... Loss: 1.2973... Val Loss: 1.3432\n",
      "Epoch: 15/20... Step: 2070... Loss: 1.3126... Val Loss: 1.3395\n",
      "Epoch: 15/20... Step: 2080... Loss: 1.2951... Val Loss: 1.3390\n",
      "Epoch: 16/20... Step: 2090... Loss: 1.3133... Val Loss: 1.3417\n",
      "Epoch: 16/20... Step: 2100... Loss: 1.2869... Val Loss: 1.3397\n",
      "Epoch: 16/20... Step: 2110... Loss: 1.2799... Val Loss: 1.3373\n",
      "Epoch: 16/20... Step: 2120... Loss: 1.2953... Val Loss: 1.3383\n",
      "Epoch: 16/20... Step: 2130... Loss: 1.2708... Val Loss: 1.3386\n",
      "Epoch: 16/20... Step: 2140... Loss: 1.2856... Val Loss: 1.3345\n",
      "Epoch: 16/20... Step: 2150... Loss: 1.3075... Val Loss: 1.3363\n",
      "Epoch: 16/20... Step: 2160... Loss: 1.2757... Val Loss: 1.3361\n",
      "Epoch: 16/20... Step: 2170... Loss: 1.2836... Val Loss: 1.3341\n",
      "Epoch: 16/20... Step: 2180... Loss: 1.2840... Val Loss: 1.3312\n",
      "Epoch: 16/20... Step: 2190... Loss: 1.2914... Val Loss: 1.3318\n",
      "Epoch: 16/20... Step: 2200... Loss: 1.2806... Val Loss: 1.3299\n",
      "Epoch: 16/20... Step: 2210... Loss: 1.2402... Val Loss: 1.3319\n",
      "Epoch: 16/20... Step: 2220... Loss: 1.2852... Val Loss: 1.3332\n",
      "Epoch: 17/20... Step: 2230... Loss: 1.2610... Val Loss: 1.3289\n",
      "Epoch: 17/20... Step: 2240... Loss: 1.2741... Val Loss: 1.3302\n",
      "Epoch: 17/20... Step: 2250... Loss: 1.2610... Val Loss: 1.3268\n",
      "Epoch: 17/20... Step: 2260... Loss: 1.2814... Val Loss: 1.3295\n",
      "Epoch: 17/20... Step: 2270... Loss: 1.2798... Val Loss: 1.3284\n",
      "Epoch: 17/20... Step: 2280... Loss: 1.2808... Val Loss: 1.3254\n",
      "Epoch: 17/20... Step: 2290... Loss: 1.2735... Val Loss: 1.3273\n",
      "Epoch: 17/20... Step: 2300... Loss: 1.2444... Val Loss: 1.3266\n",
      "Epoch: 17/20... Step: 2310... Loss: 1.2642... Val Loss: 1.3251\n",
      "Epoch: 17/20... Step: 2320... Loss: 1.2546... Val Loss: 1.3258\n",
      "Epoch: 17/20... Step: 2330... Loss: 1.2615... Val Loss: 1.3266\n",
      "Epoch: 17/20... Step: 2340... Loss: 1.2746... Val Loss: 1.3234\n",
      "Epoch: 17/20... Step: 2350... Loss: 1.2799... Val Loss: 1.3207\n",
      "Epoch: 17/20... Step: 2360... Loss: 1.2862... Val Loss: 1.3228\n",
      "Epoch: 18/20... Step: 2370... Loss: 1.2465... Val Loss: 1.3193\n",
      "Epoch: 18/20... Step: 2380... Loss: 1.2693... Val Loss: 1.3220\n",
      "Epoch: 18/20... Step: 2390... Loss: 1.2621... Val Loss: 1.3218\n",
      "Epoch: 18/20... Step: 2400... Loss: 1.2835... Val Loss: 1.3204\n",
      "Epoch: 18/20... Step: 2410... Loss: 1.2727... Val Loss: 1.3206\n",
      "Epoch: 18/20... Step: 2420... Loss: 1.2549... Val Loss: 1.3199\n",
      "Epoch: 18/20... Step: 2430... Loss: 1.2680... Val Loss: 1.3201\n",
      "Epoch: 18/20... Step: 2440... Loss: 1.2554... Val Loss: 1.3185\n",
      "Epoch: 18/20... Step: 2450... Loss: 1.2439... Val Loss: 1.3178\n",
      "Epoch: 18/20... Step: 2460... Loss: 1.2733... Val Loss: 1.3166\n",
      "Epoch: 18/20... Step: 2470... Loss: 1.2594... Val Loss: 1.3179\n",
      "Epoch: 18/20... Step: 2480... Loss: 1.2455... Val Loss: 1.3154\n",
      "Epoch: 18/20... Step: 2490... Loss: 1.2359... Val Loss: 1.3121\n",
      "Epoch: 18/20... Step: 2500... Loss: 1.2362... Val Loss: 1.3156\n",
      "Epoch: 19/20... Step: 2510... Loss: 1.2521... Val Loss: 1.3139\n",
      "Epoch: 19/20... Step: 2520... Loss: 1.2625... Val Loss: 1.3158\n",
      "Epoch: 19/20... Step: 2530... Loss: 1.2671... Val Loss: 1.3138\n",
      "Epoch: 19/20... Step: 2540... Loss: 1.2841... Val Loss: 1.3150\n",
      "Epoch: 19/20... Step: 2550... Loss: 1.2396... Val Loss: 1.3152\n",
      "Epoch: 19/20... Step: 2560... Loss: 1.2579... Val Loss: 1.3117\n",
      "Epoch: 19/20... Step: 2570... Loss: 1.2380... Val Loss: 1.3092\n",
      "Epoch: 19/20... Step: 2580... Loss: 1.2773... Val Loss: 1.3125\n",
      "Epoch: 19/20... Step: 2590... Loss: 1.2288... Val Loss: 1.3112\n",
      "Epoch: 19/20... Step: 2600... Loss: 1.2353... Val Loss: 1.3090\n",
      "Epoch: 19/20... Step: 2610... Loss: 1.2436... Val Loss: 1.3095\n",
      "Epoch: 19/20... Step: 2620... Loss: 1.2325... Val Loss: 1.3079\n",
      "Epoch: 19/20... Step: 2630... Loss: 1.2416... Val Loss: 1.3081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20... Step: 2640... Loss: 1.2514... Val Loss: 1.3102\n",
      "Epoch: 20/20... Step: 2650... Loss: 1.2499... Val Loss: 1.3060\n",
      "Epoch: 20/20... Step: 2660... Loss: 1.2494... Val Loss: 1.3122\n",
      "Epoch: 20/20... Step: 2670... Loss: 1.2683... Val Loss: 1.3096\n",
      "Epoch: 20/20... Step: 2680... Loss: 1.2459... Val Loss: 1.3094\n",
      "Epoch: 20/20... Step: 2690... Loss: 1.2454... Val Loss: 1.3096\n",
      "Epoch: 20/20... Step: 2700... Loss: 1.2499... Val Loss: 1.3063\n",
      "Epoch: 20/20... Step: 2710... Loss: 1.2219... Val Loss: 1.3081\n",
      "Epoch: 20/20... Step: 2720... Loss: 1.2226... Val Loss: 1.3066\n",
      "Epoch: 20/20... Step: 2730... Loss: 1.2220... Val Loss: 1.3035\n",
      "Epoch: 20/20... Step: 2740... Loss: 1.2202... Val Loss: 1.3017\n",
      "Epoch: 20/20... Step: 2750... Loss: 1.2199... Val Loss: 1.3050\n",
      "Epoch: 20/20... Step: 2760... Loss: 1.2171... Val Loss: 1.3023\n",
      "Epoch: 20/20... Step: 2770... Loss: 1.2565... Val Loss: 1.3011\n",
      "Epoch: 20/20... Step: 2780... Loss: 1.2699... Val Loss: 1.2999\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'rnn_20_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna, a complete of the conversation.\n",
      "\n",
      "\"I didn't know that the province of a long whole same to be done to meet you, and I will not be taken on the creak to\n",
      "be anything.\"\n",
      "\n",
      "\"You say.\n",
      "\n",
      "It was as a strick man think, I she will not\n",
      "see him. To speak of intimace from me. A don't trie terrible to be about.\"\n",
      "\n",
      "\"Oh, yes! Well, and I wanted to did you and have all the commister,\" she said to himself. \"I shill set to be soletions in his wife thanked it on the carrea to the\n",
      "concert and\n",
      "service, and so much friends is a sound of and something as\n",
      "in the\n",
      "servants.\n",
      "This wors of calling\n",
      "all times sense of a peasant and togith, that her face. They could never say about that to the sore of the same, that\n",
      "he could not his love, and then we must had to go and see that there will be any of their sunshining, and shall be seeing her face in seeing them. At the marsh had been a second being so always and the painfess to the\n",
      "study of the\n",
      "moment of their cares and a lady\n",
      "again, and he came into him and have things.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Anna', top_k=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
    "with open('rnn_20_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said,\n",
      "that a long while he had said that the\n",
      "sick man and to her sorts of clothes, showing the continual\n",
      "countess, and where this consticuanished to hume in any tark was\n",
      "nurse.\n",
      "\n",
      "Anna, to bring him an inspated on the crup of a clight,\n",
      "trusing hands, and said, and that there he seemed to\n",
      "show some side of the came\n",
      "and doing a service. The stath of wind so and saying that she was always always been for the party and and were sertaing him from the prince, and she saw\n",
      "a summer sight of the stretthe that he was dount, with which suddenly with his brother's capitill of the sofa instant the steps of which she was still an erection of her face\n",
      "and the footman.\n",
      "\n",
      "\"I am not\n",
      "giving the minute.\"\n",
      "\n",
      "\"No, it's a country sat difter, than all hands are any one alone. How all you must\n",
      "be an hare. And how don't you know! I did not go on it....\"\n",
      "\n",
      "She wanted to have so much for her\n",
      "husband; and was taking off the face, he sat the\n",
      "staricies and said,\n",
      "so went out into the driving with said of the\n",
      "death to have a single peeping on their face, and tried to tell what it was\n",
      "the backs of\n",
      "his wife, and a shart of almending the conversation. Hithly with all\n",
      "tilent\n",
      "of his face, and\n",
      "sat down.\n",
      "\n",
      "\"Yes, but I don't come in.\"\n",
      "\n",
      "\"I don't say it with the children,\" said Levin.\n",
      "\n",
      "\"Well, there's nothing\n",
      "weary, a soration of all her\n",
      "thoughts?\" he thought. \"Wold mister to talk to yourself.\"\n",
      "\n",
      "Sveazhsky admited all still to be to speak to him, and seeming his wife and\n",
      "hunded, and the marsh of the professor, a forest and trees than she\n",
      "had told him and say that his brother were brunging into the sight of the pasenter. He was the part of her head.\n",
      "\n",
      "\"Yes, I see it was the corridor and there out of the sounds, through her, and work about it all this to them along the sound and more to sat terrs. I can't believe I won't go about and shall\n",
      "be a moment, and I show it's tried to be almost an idoar feeling.\"\n",
      "\n",
      "\"Oh, I'm going into the man a sound of thousand three impersation.\"\n",
      "\n",
      "And she went on to his\n",
      "high front and started, but \n"
     ]
    }
   ],
   "source": [
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, prime=\"And Levin said\", top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
